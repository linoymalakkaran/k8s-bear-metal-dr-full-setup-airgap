# Production On-Premise Kubernetes Cluster with Rancher

## Overview
This project provides a comprehensive guide and automation scripts for deploying a production-grade on-premise Kubernetes cluster using Rancher, with full Disaster Recovery (DR) capabilities.

## Project Structure
```
dr-k8s/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ QUICKSTART.md                      # 30-minute quick start guide
â”œâ”€â”€ PROJECT_SUMMARY.md                 # Complete feature overview
â”œâ”€â”€ docs/                              # Comprehensive documentation
â”‚   â”œâ”€â”€ 00-bare-metal-setup.md         # NEW: Complete bare metal server setup
â”‚   â”œâ”€â”€ 01-infrastructure-requirements.md
â”‚   â”œâ”€â”€ 02-network-design.md
â”‚   â”œâ”€â”€ 03-installation-guide.md
â”‚   â”œâ”€â”€ 04-airgap-installation.md
â”‚   â”œâ”€â”€ airgap-images-manifest.md      # NEW: Complete airgap image inventory
â”‚   â”œâ”€â”€ master-worker-setup.md         # NEW: Master/worker/etcd setup guide
â”‚   â”œâ”€â”€ 05-disaster-recovery.md
â”‚   â”œâ”€â”€ 06-monitoring-observability.md
â”‚   â”œâ”€â”€ 07-backup-restore.md
â”‚   â””â”€â”€ 08-maintenance-operations.md
â”œâ”€â”€ infrastructure/                    # Infrastructure as Code
â”‚   â”œâ”€â”€ terraform/                     # Terraform configurations (if using)
â”‚   â”œâ”€â”€ ansible/                       # Ansible playbooks for provisioning
â”‚   â””â”€â”€ requirements.yaml              # Infrastructure requirements spec
â”œâ”€â”€ rancher/                           # Rancher setup
â”‚   â”œâ”€â”€ installation/                  # Installation scripts
â”‚   â”œâ”€â”€ config/                        # Configuration files
â”‚   â””â”€â”€ backup/                        # Backup configurations
â”œâ”€â”€ kubernetes/                        # Kubernetes configurations
â”‚   â”œâ”€â”€ cluster-config/                # Cluster configuration
â”‚   â”œâ”€â”€ namespaces/                    # Namespace definitions
â”‚   â”œâ”€â”€ rbac/                          # RBAC policies
â”‚   â””â”€â”€ network-policies/              # Network policies
â”œâ”€â”€ helm-charts/                       # Helm charts
â”‚   â”œâ”€â”€ applications/                  # Application charts
â”‚   â””â”€â”€ infrastructure/                # Infrastructure charts
â”œâ”€â”€ scripts/                           # Automation scripts
â”‚   â”œâ”€â”€ setup/                         # Setup scripts
â”‚   â”œâ”€â”€ backup/                        # Backup scripts
â”‚   â”œâ”€â”€ restore/                       # Restore scripts
â”‚   â””â”€â”€ maintenance/                   # Maintenance scripts
â”œâ”€â”€ pipelines/                         # CI/CD pipelines
â”‚   â”œâ”€â”€ jenkins/                       # Jenkins pipelines
â”‚   â”œâ”€â”€ gitlab-ci/                     # GitLab CI configurations
â”‚   â””â”€â”€ github-actions/                # GitHub Actions workflows
â”œâ”€â”€ monitoring/                        # Monitoring setup
â”‚   â”œâ”€â”€ prometheus/                    # Prometheus configurations
â”‚   â”œâ”€â”€ grafana/                       # Grafana dashboards
â”‚   â””â”€â”€ alertmanager/                  # Alert configurations
â””â”€â”€ dr/                                # Disaster Recovery
    â”œâ”€â”€ runbooks/                      # DR runbooks
    â”œâ”€â”€ scripts/                       # DR automation scripts
    â””â”€â”€ configs/                       # DR configurations
```

## Quick Start

See [QUICKSTART.md](QUICKSTART.md) for a complete 30-minute deployment guide.

### Prerequisites
- **Bare metal servers** or VMs (3 masters + 3-5 workers minimum)
- **OS**: Rocky Linux 8.8+, RHEL 8+, or Ubuntu 22.04 LTS
- **Hardware**: See [Infrastructure Requirements](docs/01-infrastructure-requirements.md)
- Root or sudo access on all nodes
- Network connectivity between nodes
- (Optional) Air-gapped environment setup

### Installation Path

**For Bare Metal Deployment:**
1. ğŸ–¥ï¸ **[Bare Metal Setup](docs/00-bare-metal-setup.md)** - Complete hardware and OS installation
   - BIOS/UEFI configuration
   - RAID setup
   - OS installation (Rocky Linux 8)
   - Disk partitioning
   - Network configuration
   
2. ğŸ—ï¸ **[Infrastructure Requirements](docs/01-infrastructure-requirements.md)** - Hardware specifications
   
3. ğŸŒ **[Network Design](docs/02-network-design.md)** - VLAN design, HAProxy, firewall rules

4. ğŸ¯ **[Master & Worker Setup](docs/master-worker-setup.md)** - Complete cluster bootstrapping
   - Bootstrap first master node
   - Setup 3-node etcd cluster
   - Join additional masters
   - Add worker nodes
   
5. ğŸ“¦ **[Installation Guide](docs/03-installation-guide.md)** - RKE2 and Rancher installation

**For Airgap Deployment:**
- ğŸ“¥ **[Airgap Images Manifest](docs/airgap-images-manifest.md)** - Complete image list with sizes
- ğŸ”’ **[Airgap Installation](docs/04-airgap-installation.md)** - Offline installation procedures

**Post-Installation:**
- ğŸ”„ **[Disaster Recovery](docs/05-disaster-recovery.md)** - DR setup and failover
- ğŸ“Š **[Monitoring & Observability](docs/06-monitoring-observability.md)** - Complete monitoring stack

## Key Features

### Production-Ready Setup
- High Availability (HA) configuration
- Multi-master setup for control plane
- Load balancing for API server
- etcd backup and restore capabilities

### Disaster Recovery
- Automated backup solutions
- Multi-region DR strategy
- RTO/RPO optimization
- Failover automation

### Security
- Network policies and segmentation
- RBAC implementation
- Pod Security Policies/Standards
- Secrets management
- Certificate management

### Monitoring & Observability
- Prometheus metrics collection
- Grafana dashboards
- Centralized logging (ELK/Loki)
- Alerting and notifications

### Airgap Support
- Complete offline installation guide
- Private container registry setup
- Helm chart repository hosting
- Image synchronization strategies

## Architecture

### Production Cluster Design
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Load Balancer (HAProxy/Nginx)          â”‚
â”‚                    (VIP: 192.168.1.100)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                 â”‚                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚ Master Node 1â”‚  â”‚ Master Node 2â”‚  â”‚ Master Node 3â”‚
        â”‚  (Control)   â”‚  â”‚  (Control)   â”‚  â”‚  (Control)   â”‚
        â”‚  + etcd      â”‚  â”‚  + etcd      â”‚  â”‚  + etcd      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                 â”‚                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Worker Node 1â”‚  â”‚ Worker Node 2â”‚  â”‚ Worker Node 3â”‚ â”‚ Worker Node Nâ”‚
â”‚ (Workloads)  â”‚  â”‚ (Workloads)  â”‚  â”‚ (Workloads)  â”‚ â”‚ (Workloads)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DR Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Primary Data Center (DC1)       â”‚     â”‚    DR Data Center (DC2)             â”‚
â”‚                                      â”‚     â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Production K8s Cluster        â”‚ â”‚     â”‚  â”‚   DR K8s Cluster               â”‚â”‚
â”‚  â”‚   - 3 Master Nodes              â”‚ â”‚     â”‚  â”‚   - 3 Master Nodes             â”‚â”‚
â”‚  â”‚   - N Worker Nodes              â”‚ â”‚â”€â”€â”€â”€â–ºâ”‚  â”‚   - N Worker Nodes             â”‚â”‚
â”‚  â”‚   - Rancher Management          â”‚ â”‚     â”‚  â”‚   - Rancher Management         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                      â”‚     â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Storage (Persistent)          â”‚ â”‚     â”‚  â”‚   Storage (Replicated)         â”‚â”‚
â”‚  â”‚   - Longhorn/Rook-Ceph          â”‚â—„â”œâ”€â”€â”€â”€â–ºâ”‚  â”‚   - Longhorn/Rook-Ceph         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                                             â–²
              â”‚                                             â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ etcd Backup / Replication â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Infrastructure Requirements Summary

### Minimum Production Setup
- **Master Nodes**: 3 nodes (HA)
  - 4 vCPU, 16GB RAM, 100GB SSD each
- **Worker Nodes**: 3+ nodes (scalable)
  - 8 vCPU, 32GB RAM, 200GB SSD each
- **Load Balancer**: 2 nodes (HA)
  - 2 vCPU, 4GB RAM, 50GB disk each
- **Storage**: Distributed storage solution (Longhorn/Rook-Ceph)
  - Additional disks for persistent storage

See detailed requirements in [Infrastructure Requirements](docs/01-infrastructure-requirements.md)

## Support & Maintenance

### Regular Maintenance Tasks
- etcd backup verification
- Certificate renewal
- Security updates
- Cluster upgrades
- Capacity planning

### Monitoring Endpoints
- Rancher UI: `https://rancher.example.com`
- Grafana: `https://grafana.example.com`
- Prometheus: `https://prometheus.example.com`

## Contributing
This is an internal production infrastructure project. Please follow the change management process for any modifications.

## License
Internal Use Only - [Your Organization]

## Authors
- Infrastructure Team
- DevOps Team

## Last Updated
January 2026
# k8s-bear-metal-dr-full-setup-airgap
